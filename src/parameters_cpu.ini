[Main]
experiment_name = RoBERTa
no_cuda = True
; "model_name = RoBERTa or LLMv3"
model_name = RoBERTa
early_stopping_patience = 99
reduce_lr_on_plateau_patience = 10

[QAModel]
lr = 2e-5
; "scheduler_mode = {0: None, 1: "lighting-auto", 2: "ReduceLROnPlateau"}"
lr_scheduler_mode = 0
; "optimizer = ranger or adam"
optimizer = adam
log_train_every_n_steps = 1
log_val_every_n_steps = 1
log_test_every_n_steps = 1
; checkpoint = runs\test\version_4\checkpoints\epoch=2-step=15.ckpt

[SquadDataset]
include_references = False
; "html_version = online or <year> e.g., 2017"
html_version = 2017
threshold_min = 1
threshold_max = 1
dev_ratio = 0.1 
test_ratio = 0.1
batch_size = 2
image_width = 224
image_height = 224
tokenizer_max_length = 300 # keep it small
tokenizer_stride = 256
; split_seed = 10 
; dataloader_seed = 100
num_workers = 4
is_layout_dependent = True

[PytorchLightning]
max_epochs = 2
limit_train_batches = 2
limit_val_batches = 2
limit_test_batches = 2
num_sanity_val_steps = 0