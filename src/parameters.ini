[Main]
experiment_name = test_lr_plateau
no_cuda = True
; "model_name = RoBERTa or LLMv3"
model_name = RoBERTa
early_stopping_patience = 99
reduce_lr_on_plateau_patience = 10

[QAModel]
lr = 2e-5
; "scheduler_mode = {0: None, 1: "lighting-auto", 2: "ReduceLROnPlateau"}"
lr_scheduler_mode = 0
; "optimizer = ranger or adam"
optimizer = ranger 
log_train_every_n_steps = 1
log_val_every_n_steps = 1
log_test_every_n_steps = 1
; checkpoint = runs\test\version_4\checkpoints\epoch=2-step=15.ckpt

[SquadDataset]
include_references = False
; "html_version = online or <year> e.g., 2017"
html_version = online
scale_factor = 1 
threshold_min = 1
threshold_max = 1
max_token_length = 512
ignore_impossible = True
; "para_asset_type = page_width_fit_para_box, para_box, or whole_para_page"
para_asset_type = para_box 
dev_split = 0.1
train_split = 0.2
batch_size = 2
image_size_llmv3 = 224 
llmv3_max_length = 512 
roberta_max_length = 512 
llmv3_stride = 50
roberta_stride = 50 
; split_seed = 10
; dataloader_seed = 15
num_workers = 4

[PytorchLightning]
; gpus = 1
max_epochs = 1
; "steps = batches"
limit_train_batches = 2
limit_val_batches = 2
limit_test_batches = 1
num_sanity_val_steps = 0