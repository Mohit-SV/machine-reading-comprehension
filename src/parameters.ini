[Main]
experiment_name = LLMv3-page-docs-dr01-tr01-ts256-b8-tb500-vb50-ttb50-20ep-16p-ml300-s150-acc4
no_cuda = True
; "model_name = RoBERTa or LLMv3"
model_name = RoBERTa
early_stopping_patience = 99
reduce_lr_on_plateau_patience = 10

[QAModel]
lr = 2e-5
; "scheduler_mode = {0: None, 1: "lighting-auto", 2: "ReduceLROnPlateau"}"
lr_scheduler_mode = 0
; "optimizer = ranger or adam"
optimizer = adam
log_train_every_n_steps = 1
log_val_every_n_steps = 1
log_test_every_n_steps = 1
; checkpoint = runs\test\version_4\checkpoints\epoch=2-step=15.ckpt

[VisualSquadDataModule]
include_references = False
; "html_version = online or <year> e.g., 2017"
html_version = 2017
threshold_min = 1
threshold_max = 1
dev_ratio = 0.1 
test_ratio = 0.1
batch_size = 2
image_width = 224
image_height = 224
tokenizer_max_length = 300
tokenizer_stride = 150
; split_seed = 10 
dataloader_seed = 100
; "num_workers = 4*num_GPU"
num_workers = 4
; pin_memory=True
is_layout_dependent = False

[PytorchLightning]
; gpus = 1
max_epochs = 1
limit_train_batches = 1
limit_val_batches = 1
limit_test_batches = 2
num_sanity_val_steps = 0
; precision = 16
; accumulate_grad_batches = 4
;e.g., 4 x 8 (batch_size) = 32 (effective batch_size)